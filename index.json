[{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/langevin/","section":"Tags","summary":"","title":"Langevin","type":"tags"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/mcmc/","section":"Tags","summary":"","title":"MCMC","type":"tags"},{"content":" MCMC # 参考： https://zhuanlan.zhihu.com/p/37121528 https://medium.com/@WeiranLin/acceptance-rejection-sampling-method-illustrated-with-python-0552c0eb27d2 https://chatgpt.com/share/688ce531-d530-8011-9afc-b76d072f1215\n本文假设你已经了解了马尔可夫链的基本定义、平稳分布的含义。\n动机 # 我在学这个算法之前，参考了许多网上的教程，结果上来就是数学公式，看到一半还不知道自己在解决什么样的问题。这部分把要解决什么样的问题这件事情说清楚。\n我们要知道，现实世界中的很多概率分布是不太好估计的，并非所有概率分布都长得像高斯分布那样优雅。比如下面的分布：\nMade by Geogebra https://www.geogebra.org/calculator/qfjrjj6v. Example from FunInCode, Bilibili\n绿色线这个关于 $x$ 的分布，就问你不看表达式的情况下怎么求它的分布 $P(X)$？求不了一点吧。\n上面这种纯恶心人的分布还是一维的，更别提现实生活中更复杂的分布，例如生成式模型任务让你求一张图片 256 个像素点的联合分布 $P(x_1,x_2,…,x_{256})$ 🤮。我们不得不找一种“用数据去估计分布”的方法。\n但其实，我们也没有必要把分布的函数形式给求出来。很多分布是没有好看的函数形式的，也可能根本求不出来。我们想要的只是“从分布中采样”这么一件事情。我们可以不知道上面绿色这个分布的形式，但却可以从绿色这个分布中 sample 出很多样本 $x_1,x_2,…,x_n$。这就很 nice！以生成式模型为例，假设我们在不知道“麦晓雯美图”分布是什么的情况下，依然可以从“麦晓雯美图”分布中采样出符合这个分布的 256 个像素点 $(x_1,x_2,…,x_{256})$，那这个生成式任务不就完成了吗？\nMCMC 采样 # 平稳分布、详细平衡条件 # 那么先哲们是如何通过马尔科夫链，去跟采样这件事情联想起来的呢？\n其实马尔可夫链本身就可以看作是对概率分布 $P$ 的建模。举个很简单的例子，我们要对北京 A、上海 B三个地方的人口密度分布进行建模。当前的概率分布为 $\\pi(A)=0.6, \\pi(B)=0.4$，并假设人口在城市间的转移概率服从马尔可夫链：\n当前城市 → 下一个城市 A B A 0.7 0.3 B 0.5 0.5 那我们想问：下一步 A 点的分布是什么？\n好算！$π(A)=π(A)⋅P(A→A)+π(B)⋅P(B→A)=0.6⋅0.7+0.4⋅0.5=0.42+0.2=0.62$\n原先的分布里 A 的概率是 0.6，现在变成了 0.62，依然不平稳。但根据遍历定理，再过几步，A 和 B 的概率分布一定可以达到平稳状态。\n从上面的例子中我们提出几个符号：\n$\\pi(x)$：表示当前在状态 x 的概率（这是你当前的分布） $P(x \\to x\u0026rsquo;)$：从状态 x 跳到状态 x′ 的转移概率 所以 $\\pi(x) P(x \\to x\u0026rsquo;)$：表示你现在在 x，然后走到 x′的整体概率流量。遍历所有能走到 $x\u0026rsquo;$ 的路，就是 $\\Sigma \\pi(x)P(x \\to x\u0026rsquo;) ,\\forall x$ ，这个式子表示下一个时间步里 $x\u0026rsquo;$ 的分布情况，如果能够与 $\\pi(x\u0026rsquo;)$——也就是当前 x’的分布相等，那么说明什么？\n说明“再走一步，分布不变”，说明我们达到了平稳分布！\n而详细平衡条件是一个更强的条件，能推出平稳分布。如果我们有从北京到上海的人流，与从上海到北京到人流相等，——也就是任意两个点之间双向的流量相等，——那么就说明系统达到平稳分布了。\n$$ \\pi(x)P(x \\to x')=\\pi(x')P(x'\\to x) $$以上就是详细平衡条件。\n从前面那个公式：\n$π(x\u0026rsquo;)=∑xπ(x)P(x→x\u0026rsquo;)\\pi(x\u0026rsquo;) = \\sum{x} \\pi(x) P(x \\to x\u0026rsquo;)$\n如果每一对 $(x, x\u0026rsquo;)$ 都满足 $\\pi(x) P(x \\to x\u0026rsquo;) = \\pi(x\u0026rsquo;) P(x\u0026rsquo; \\to x)$\n那么就可以替换右边成：\n$\\sum_{x} \\pi(x\u0026rsquo;) P(x\u0026rsquo; \\to x) = \\pi(x\u0026rsquo;) \\sum_{x} P(x\u0026rsquo; \\to x) = \\pi(x\u0026rsquo;)$（因为对马尔科夫链来说，$\\sum_{x} P(x\u0026rsquo; \\to x) = 1$）\n所以，详细平衡 ⇒ 平稳分布。\nM-H MCMC # MH 的厉害之处就是：\n通过一个简单的“接受 - 拒绝”机制，构造出满足详细平衡的马尔科夫链，从而让我们能从目标分布 $\\pi(x)$ 采样。\n基本概念有：\n🎯目标分布 $\\pi(x)$。我们的目标分布 ${\\pi(x)}$ 可以是一个很复杂的分布，不需要知道归一化常数 $Z$。 ❔提议分布 $q(x\u0026rsquo;|x)$。表示当前状态 x 下，打算怎么样跳到 x’。 有了提议分布，我们却并不打算接受它。我们以概率 $\\alpha=min(1, \\frac{\\pi(x\u0026rsquo;)q(x|x\u0026rsquo;)}{\\pi(x)q(x\u0026rsquo;|x)})$ 的概率接受它。 很懵，我来告诉你为什么这么做。\n现在我们的转移概率变成了 $P(x \\to x\u0026rsquo;)=q(x\u0026rsquo;|x)\\alpha$，乘上当前的状态 $\\pi(x)$ 就是下一个时刻从 x 到 x’的“流量”：\n$π(x)P(x→x\u0026rsquo;)=π(x)q(x\u0026rsquo;∣x)α(x,x\u0026rsquo;)=min(π(x)q(x\u0026rsquo;∣x),π(x\u0026rsquo;)q(x∣x\u0026rsquo;))$\n观察到**这个式子是对称的！也就是说下一个时刻从 x’到 x 的“流量”，依然等于右式。**想想之前“详细平稳”的条件——任意两个点之间双向的流量相等，我们成功证明了按照 M-H 方法，系统达到了详细平衡，也达到了平稳分布。\nLangevin MCMC # 人工智能专业的学生表示 Langevin 方程懂不了一点。我直接复制 GPT 了：\n朗之万动力学（Langevin Dynamics）采样是非常有趣且强大的采样方法，尤其适合于连续分布和物理模拟。我们可以将它视为一种基于物理系统的马尔科夫链蒙特卡洛（MCMC）方法，它使用了梯度信息来引导采样，从而能够更加高效地在复杂分布中采样。\n🧠 朗之万动力学采样的基本思路 # 朗之万动力学采样（Langevin sampling）是通过模拟物理系统中的粒子运动来获得目标分布的样本。这个过程受到以下两方面的影响：\n势能（Potential energy）：它是目标分布的对数值，用来定义粒子的行为。 噪声：模拟粒子的随机运动，用来引入随机性。 这类似于温度控制的物理过程，其中粒子在潜在势场中受力，并且每次迭代都受到随机扰动。\n🔬 朗之万方程（Langevin Equation） # 在经典物理中，朗之万方程描述了粒子如何在势能场中运动。其数学表达式如下：\n$$ \\frac{dx}{dt} = - \\nabla U(x) + \\sqrt{2 \\gamma} \\, \\eta(t) $$其中：\nx 是粒子的状态（类似于 MCMC 中的采样点）， U(x) 是势能（在采样中，它通常是目标分布的负对数）， γ 是摩擦系数（决定粒子运动的阻力）， η(t) 是一个高斯白噪声，代表随机扰动。 🧑‍🔬 离散化的朗之万动力学采样 # 由于朗之万方程是连续的，我们通常需要将其离散化以便在计算机上实现。在离散化时，我们使用欧拉方法：\n$$ x_{t+1}=x_t−ϵ∇U(x_t)+2ϵγ ξx_{t+1} = x_t - \\epsilon \\nabla U(x_t) + \\sqrt{2 \\epsilon \\gamma} \\, \\xi_t $$其中：\n$\\epsilon$ 是步长（控制每一步的更新幅度）， $\\nabla U(x_t)$ 是目标分布（势能）的梯度，似乎只有这个是比较重要的 $\\xi_t$ 是标准正态分布的随机噪声（每一步的随机扰动）。 在 Energy-based model 的语境下：\n$$ p(x)=\\frac{e^{-U(x)}}{Z} $$能量\n$$ U(x)=-\\log p(x) $$我们已知条件是 score-function\n$$ s_\\theta(x) \\approx \\nabla_x \\log p(x) $$因此在 Langevin MCMC 中\n$$ \\nabla_x U(x_t)=-\\nabla_x \\log p(x) $$对应了“概率往梯度上升的方向走”。\n🎯 步骤概述：朗之万采样算法 # 初始化：选择一个起始点 $x_0$。 计算梯度：计算目标分布的梯度 $\\nabla U(x_t)$。 更新状态：根据离散化的朗之万方程更新状态 $x_{t+1}$。 重复：重复多次，直到收集足够的样本。 🧑‍💻 代码实现（伪代码示例） # import numpy as np import matplotlib.pyplot as plt ## 目标分布：标准正态分布的负对数 def target_distribution(x): return 0.5 * x**2 ## U(x) = (1/2) * x^2 ## 梯度：目标分布的梯度 def grad_target_distribution(x): return x ## U\u0026#39;(x) = x ## 朗之万动力学采样 def langevin_dynamics(n_samples, step_size=0.1, gamma=0.1): ## 初始状态 x = 0 samples = [x] for _ in range(n_samples - 1): ## 计算梯度 grad_U = grad_target_distribution(x) ## 添加噪声（高斯扰动） noise = np.random.normal(0, np.sqrt(2 * step_size * gamma)) ## 更新状态 x = x - step_size * grad_U + noise ## 记录样本 samples.append(x) return np.array(samples) ## 采样 10000 次 samples = langevin_dynamics(10000) ## 绘制直方图 plt.hist(samples, bins=50, density=True, alpha=0.6, color=\u0026#39;b\u0026#39;) x = np.linspace(-4, 4, 100) plt.plot(x, np.exp(-target_distribution(x)), \u0026#39;r\u0026#39;, lw=2) ## 目标分布的密度函数 plt.title(\u0026#39;Langevin Dynamics Sampling\u0026#39;) plt.show() 📚 解释代码： # 目标分布：我们选了一个简单的标准正态分布的负对数（势能）作为目标分布 $U(x)。$ 这其实是 $\\pi(x) = \\mathcal{N}(0, 1)$ 的负对数。 梯度计算：计算 $U(x)$ 的梯度（对于 $U(x) = \\frac{1}{2} x^2$ 来说，梯度就是 $\\nabla U(x) = x$）。 更新状态：每次更新状态时，除了梯度项外，还加了一个来自高斯噪声的扰动项，确保状态更新是随机的。 绘制直方图：运行采样并绘制直方图，看看朗之万采样生成的样本是否接近目标分布。 🎯 优点与应用 # 高效采样：朗之万采样可以高效地在复杂的目标分布中采样，尤其适用于有梯度信息的情形。 物理模型模拟：广泛应用于分子动力学模拟、粒子系统、统计物理等领域。 与 MH 对比：相较于 Metropolis-Hastings，朗之万采样通过使用梯度信息，可以在某些情况下更快收敛，并且更好地探索复杂的多峰分布。 ","date":"5 August 2025","externalUrl":null,"permalink":"/posts/ai/aigc/awesome-blogs/mcmc-markov-chain-monte-carlo/","section":"Posts","summary":"","title":"MCMC (Markov Chain Monte Carlo)","type":"posts"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/","section":"Niyuta's Blog","summary":"","title":"Niyuta's Blog","type":"page"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" 训练常用命令 # Scp # 好处在于 windows 上也自带这个命令，而且很简单，只需记住 -r 参数就可以了。\n坏处是小文件特别多的话比较慢。这种情况下建议主用 rysnc 。\n为什么最好不要用 git 呢？首先自己写的 private 仓库代码用 git 不好在服务器上同步，其次 git lfs 也会占用大空间（ du -h ./ 查看），再者部署用 git 也实在不优雅。一般部署是用 scp 传二进制或者 rsync 传 python 这种零散脚本。\n递归复制目录：\nscp -r my_directory user@remote_host:/home/user/directory 复制本地的文件到远程。反之从远程到本地的话就改一改。\n不需要对文件在远程再命名 ( •̀ ω •́ ) 是直接就放进对应目录里\nscp -P 35394 *pt user@remote.com:/root/models/ ## 本地的*.pt文件 scp -P 35394 model1.pt model2.pt user@remote.com:/root/models/ ## 可以拷贝多个文件 常用参数 rpvC ：\nr：递归复制整个目录。 P：指定远程主机的 SSH 端口号（默认是 22）。注意， P 是大写 p：保留文件的修改时间、访问时间和权限。 v：显示详细的调试信息，有助于排查问题。 C：启用压缩，可以加快传输速度。 Rsync # 这个和 scp 命令起到的作用差不多，但是支持断点续传、增量传输。但是这个命令 windows 上没有。\n如果 ssh 命令有附加的参数，则必须使用 -e 参数指定所要执行的 SSH 命令。\n最终版命令 # 一般最常用的选项组合 -avzP 来进行传输。支持同步增量传输、改端口、忽略隐藏文件：\nrsync -avzP -e \u0026#39;ssh -p 2234\u0026#39; --exclude=\u0026#39;.*\u0026#39; source/ user@remote_host:/destination 如果要排除一些文件夹（比如说 .venv ），用 --exclude 。\n这里有个坑 路径是自动相对于传输目录的正则，下面的例子如果写 --exclude='./models 就会完全匹配不上。\n## 支持通配符，用于删除mac finder下一些不可名状的小东西和.venv之伦 rsync -avzP --exclude=\u0026#39;.*\u0026#39; --exlude=\u0026#39;models*\u0026#39; /local/path/ user@remote:/remote/path/ 参数 # a、-archive 参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。 -append 参数指定文件接着上次中断的地方，继续传输。 -append-verify 参数跟 -append 参数类似，但会对传输完成后的文件进行一次校验。如果校验失败，将重新发送整个文件。 b、-backup 参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由 -suffix 参数指定的文件后缀名，默认是 ~。 -backup-dir 参数指定文件备份时存放的目录，比如 -backup-dir=/path/to/backups。 -bwlimit 参数指定带宽限制，默认单位是 KB/s，比如 -bwlimit=100。 c、-checksum 参数改变 rsync 的校验方式。默认情况下，rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；使用这个参数以后，则通过判断文件内容的校验和，决定是否重新传输。 -delete 参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。（有风险！模型训练完，如果加这个参数的话直接没了） e 参数指定使用 SSH 协议传输数据。 -exclude 参数指定排除不进行同步的文件，比如 -exclude=\u0026quot;*.iso\u0026quot;。 -exclude-from 参数指定一个本地文件，里面是需要排除的文件模式，每个模式一行。 -existing、-ignore-non-existing 参数表示不同步目标目录中不存在的文件和目录。 h 参数表示以人类可读的格式输出。 h、-help 参数返回帮助信息。 i 参数表示输出源目录与目标目录之间文件差异的详细情况。 -ignore-existing 参数表示只要该文件在目标目录中已经存在，就跳过去，不再同步这些文件。 -include 参数指定同步时要包括的文件，一般与 -exclude 结合使用。 -link-dest 参数指定增量备份的基准目录。 m 参数指定不同步空目录。 -max-size 参数设置传输的最大文件的大小限制，比如不超过 200KB（-max-size='200k'）。 -min-size 参数设置传输的最小文件的大小限制，比如不小于 10KB（-min-size=10k）。 n 参数或 -dry-run 参数模拟将要执行的操作，而并不真的执行。配合 v 参数使用，可以看到哪些内容会被同步过去。 P 参数是 -progress 和 -partial 这两个参数的结合。 -partial 参数允许恢复中断的传输。不使用该参数时，rsync 会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与 -append 或 -append-verify 配合使用。 -partial-dir 参数指定将传输到一半的文件保存到一个临时目录，比如 -partial-dir=.rsync-partial。一般需要与 -append 或 -append-verify 配合使用。 -progress 参数表示显示进展。 r 参数表示递归，即包含子目录。 -remove-source-files 参数表示传输成功后，删除发送方的文件。 -size-only 参数表示只同步大小有变化的文件，不考虑文件修改时间的差异。 -suffix 参数指定文件名备份时，对文件名添加的后缀，默认是 ~。 u、-update 参数表示同步时跳过目标目录中修改时间更新的文件，即不同步这些有更新的时间戳的文件。 v 参数表示输出细节。vv 表示输出更详细的信息，vvv 表示输出最详细的信息。 -version 参数返回 rsync 的版本。 z 参数指定同步时压缩数据。 --delete 删除本机没有，远程有的文件。 Tmux # https://www.ruanyifeng.com/blog/2019/10/tmux.html\n极简流程 # 新建会话 tmux new -s my_session。或者直接 tmux 进入一个从 0 开始自然编号的 session。 在 Tmux 窗口运行所需的程序。 按下快捷键 Ctrl+b d 将会话分离。 下次使用时，重新连接到会话 tmux a -t my_session 或者 tmux attach-session -t my_session。 支持鼠标滚动 # ctrl+b 进入命令面板，在命令行处输入 : ，然后输入 set -g mouse on 按回车即可\n保存输出 # tmux capture-pane -p -S - -E - \u0026gt; output.txt S -：表示从缓冲区的最开始（最顶端）开始捕获。 E -：表示捕获到缓冲区的末尾（最底端）。如果输出太多，且缓冲区设置有限，早期的输出会被丢弃，不会在这里捕获到。 p：打印捕获内容到标准输出，配合重定向保存文件。 使用 tmux set-option -g history-limit 50000 把缓存区历史记录增大到 50000 条\n其他常用命令 # tmux 快速进入一个 session。从 0 开始编号。 tmux ls 列出所有 session 终止当前窗格的命令： ctrl+b ， x Ln # 创建链接。\n一般用 rsync 命令把代码文件传到 ~ 目录下，然后用 scp 或 jupyter-lab 或 oss 或其他随便什么东西，把模型文件传到 /hy-tmp 下，最后用软链接结合它们。\n比如恒源云， /hy-tmp 文件夹是 SSD，读写速度最快，那么就需要把当前文件夹下的数据集先移动到 /hy-tmp 再创建软连接：\nmv -p ./dataset /hy-tmp/niyuta/dataset \u0026amp;\u0026amp; ln -s /hy-tmp/niyuta/dataset dataset 这里也有坑 如果后面 ln 写的是 ln -s /hy-tmp/niyuta/dataset ./dataset 就会报错本地没有这个文件夹。其实仔细想想就能明白， dataset 是一个符号而已，将其指派为文件夹显然不对。\nNote 如果后面 ln 写的是 ln -s /hy-tmp/niyuta/dataset ./dataset 就会报错本地没有这个文件夹。其实仔细想想就能明白， dataset 是一个符号而已，将其指派为文件夹显然不对。\n","date":"5 August 2025","externalUrl":null,"permalink":"/posts/ai/engineer/%E5%9C%A8%E4%BA%91gpu%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%9E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E5%BF%85%E4%BC%9A%E7%9A%84%E5%87%A0%E4%B8%AA%E5%91%BD%E4%BB%A4/","section":"Posts","summary":"","title":"在云GPU环境下搞深度学习训练，必会的几个命令","type":"posts"},{"content":" Score Matching # 我觉得 Score matching by Andy Jones 说得挺清楚了，所以这一篇笔记主要是回忆性质，记下几个 key point 和 key idea。\nScore-matching 要解决的问题是？ 如何理解 Score 函数？Score 函数是如何解决 normalizing constant 的问题的？ 如何衡量两个分布的梯度的差距？为什么用 Fisher 散度？ 为什么涉及到真实分布 $p_d(x)$ 的就不好求？ 🙋什么是 Score Matching？ # Score Matching Score Matching 是一种用于拟合统计模型的方法，特别适用于处理不可计算归一化常数（intractable normalizing constants）的模型。在机器学习中，当模型的似然函数复杂且难以归一化时（例如在能量基模型（EBMs）、生成对抗网络（GANs）或变分自编码器（VAEs 中），Score Matching 提供了一种绕过归一化常数的方法来优化模型参数。\n假设我们有一组观测数据 $x_1, \\dots, x_n$，这些数据服从未知的真实分布 $p_d(x)$。我们希望用一个参数化的模型 model 分布 $p_m(x; \\theta)$ 来近似 distribution $p_d(x)$，其中 $\\theta$ 是模型参数。目标是找到合适的 $\\theta$，使 $p_m(x; \\theta)$ 尽可能接近 $p_d(x)$。\n在最大似然估计（MLE）中，我们通常通过最大化数据的对数似然来优化 $\\theta$：\n$$ \\widehat{\\theta}_{MLE} = argmax_{\\theta} \\log p_m(x; \\theta). $$模型的概率密度函数通常可以写成未归一化的密度 $\\widetilde{p}(x; \\theta)$ 和归一化常数 $Z_\\theta$ 的形式：\n$$ p_m(x; \\theta) = \\frac{\\widetilde{p}(x; \\theta)}{Z_\\theta}, \\quad Z_\\theta = \\int_{\\mathcal{X}} \\widetilde{p}(x; \\theta) , dx. $$这里的 $Z_\\theta$ 是一个 normalizing constant，通常在复杂模型中难以计算（即不可解，intractable）。Score Matching 的核心思想是通过避免直接计算 $Z_\\theta$，来解决这一问题。\n我觉得这里完全有必要说一下：难在哪儿？为什么不能求出 $p_d(x)$？ Note 因为真实分布只有样本，没有分布，我们可以假设模型分布 $p_m$ 为高斯啦学生分布啦等等，但是真实分布通常有可能是高斯混合模型，很难搞。所以我们得想办法消除 $p_d(x)$ 这一项。在后文中还会出现一次 $p_{m}$ 项，留意之。\n💡Score Matching 的核心思想 # 其实我们如果注意到，对上面的 model 分布 p 对 x 求梯度，那么 Z 这一项就会消失（因为参数里没有 x）。\n在 Score Matching 中，==score 函数==是指对数似然函数关于数据 $x$ 的梯度：\n$$ s_{\\theta}=\\nabla_x \\log p_m(x; \\theta). $$将其展开，我们可以看到归一化常数的优势：\n$$ \\nabla_x \\log p_m(x; \\theta) = \\nabla_x \\log \\widetilde{p}_m(x; \\theta) - \\nabla_x \\log Z_\\theta. $$由于 $Z_\\theta$ 不依赖于 $x$，其梯度 $\\nabla_x \\log Z_\\theta = 0$，因此：\n$$ \\nabla_x \\log p_m(x; \\theta) = \\nabla_x \\log \\widetilde{p}_m(x; \\theta). $$Normalizing constant Z 消失了！那么 Score matching 的核心思想变呼之欲出：如果建模分布 $p_m$ 与原始分布 $p_d$ 相似，那么他们的梯度也应该相似。最后顶多差一个偏移而已。那么我们不求（或者以后再想办法求）$p(x)$，而是去求 $p_{x}$ 对数的梯度（Score-function），让 score-function 尽可能逼近真实数据的对数梯度。这个就是 Score-matching 方法。\n🎯Score Matching 的目标 # Score Matching 的目标是最小化模型分布的 score 函数与真实数据分布的 score 函数之间的 Fisher 散度（Fisher Divergence）：\n$$ \\widehat{\\theta}_{SM} = argmin_{\\theta} D_F(p_d, p_m) = argmin_{\\theta} \\frac{1}{2} \\mathbb{E}_{p_d} \\left[ | \\nabla_x \\log p_d(x) - \\nabla_x \\log p_m(x; \\theta) |_2^2 \\right]. $$ 这是因为只有 fisher 散度可以跟梯度联系起来，而 KL 散度是做不到的。 但到了这里还是有 $p_d(x)$ 项，还是不好求。所以自然地想到下一步要怎么做。\n↻绕过归一化常数和数据分布 # 展开 Fisher 散度： $$ \\frac{1}{2} | \\nabla_x \\log p_d(x) - \\nabla_x \\log p_m(x; \\theta) |_2^2 = \\frac{1}{2} (\\nabla_x \\log p_d(x))^2 - \\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) + \\frac{1}{2} (\\nabla_x \\log p_m(x; \\theta))^2. $$ 第一项 $\\frac{1}{2} (\\nabla_x \\log p_d(x))^2$ 是常数项，不依赖 $\\theta$，不影响优化 $\\theta$，可以忽略。 第三项 $\\frac{1}{2} (\\nabla_x \\log p_m(x; \\theta))^2$ 可以通过数据样本直接估计，因为它不依赖 $p_d(x)$。 处理交叉项： $$ \\mathbb{E}_{p_d} \\left[ -\\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) \\right] = -\\int_{-\\infty}^{\\infty} \\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) p_d(x) , dx. $$通过分部积分法（integration by parts）（这里很巧妙但是我不细写了），假设边界项在无穷远处消失（即 $p_d(x) \\nabla_x \\log p_m(x; \\theta) \\to 0$ 当 $|x|_2 \\to \\infty$），我们可以将交叉项转化为：\n$$ \\int_{-\\infty}^{\\infty} \\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx. $$那么问题来了：==为什么会假设边界项消失？==\n边界项消失 边界项 $p_d(x) \\nabla_x \\log p_m(x; \\theta) \\to 0$ 当 $|x|_2 \\to \\infty$ 是一个正则化条件，确保分部积分成立。直观上：\n数据分布 $p_d(x)$ 通常在无穷远处迅速衰减到零，因为实际数据集中在有限区域，尾部概率很小。例如高斯分布 $p_d(x) \\propto \\exp(-x^2 / (2\\sigma_d^2))$ 的尾部以指数速度衰减。 模型 score 函数 $\\nabla_x \\log p_m(x; \\theta)$ 通常增长较慢（例如高斯模型中为线性增长）。因此，乘积 $p_d(x) \\nabla_x \\log p_m(x; \\theta)$ 在无穷远处趋于零，因为数据分布的尾部衰减比 score 函数的增长快。 这就像在积分中，尾部贡献变得微不足道，因为数据分布在无穷远处几乎没有概率质量。 在高斯分布的例子中：\n$$ p_d(x)\\nabla_x \\log p_m(x;\\mu,\\sigma^2) \\sim \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma_d^{2}}\\right)\\cdot\\frac{\\mu-x}{\\sigma^{2}} \\to 0 $$因为指数衰减比线性增长快得多。\n因为 $\\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx$ 在大于等于 2 阶情况下的其实是个 Hessian 矩阵，对角线的元素才是对 x 求二阶导，所以写成 tr 的形式 $$ \\int_{-\\infty}^{\\infty} \\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx = \\mathbb{E}_{p_d} \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x; \\theta) \\right) \\right] $$所以损失函数：\n$$ D_F(p_d, p_m) \\propto L(\\theta) = \\mathbb{E}_{p_d} \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x; \\theta) \\right) + \\frac{1}{2} ||\\nabla_x \\log p_m(x; \\theta)||_2^2 \\right]. $$使用数据样本 $x_1, \\dots, x_n$，目标函数可近似为：\n$$ L(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x_i; \\theta) \\right) + \\frac{1}{2} |\\nabla_x \\log p_m(x_i; \\theta)|_2^2 \\right]. $$这个目标函数完全不依赖归一化常数 $Z_\\theta$ 和真实分布 $p_d(x)$，只需要模型的未归一化密度 $\\widetilde{p}_m(x; \\theta)$ 和数据样本即可。\nHyvärinen 证明，如果真实分布 $p_d(x) = p_m(x; \\theta^\\star)$ 属于模型族，则优化 $L(\\theta)$ 可找到最优参数 $\\theta^\\star$。\n只要真实分布 恰好能被模型族中的某个参数 $\\theta^\\star$ 表示出来，那么最小化得分匹配目标 $L(θ)$ 就一定能把这个 $\\theta^\\star$ 找出来。\n🤔目标函数的直观理解 # 目标函数由两部分组成：\nNorm 项：$\\frac{1}{2} |\\nabla_x \\log p_m(x_i; \\theta)|_2^2$ 表示模型 score 函数的大小。 当数据点 $x_i$ 被模型很好地解释时（即位于似然的高概率区域），score 函数的值较小（似然变化平缓）。 直观上，这个项希望模型的似然函数在数据点附近平滑。 Hessian 项：$\\text{tr} \\left( \\nabla_x^2 \\log p_m(x_i; \\theta) \\right)$ 表示对数似然的二阶导数（Hessian 矩阵的迹）。 如果数据点位于“尖锐”的局部极小值处（Hessian 迹为负且绝对值较大），说明模型对数据的解释更“独特”。 直观上，这个项倾向于选择更“尖锐”的似然函数，避免过于平坦的似然（平坦的似然意味着多种参数值都能解释数据）。 这两个项相互平衡：Norm 项倾向于平滑的似然，Hessian 项倾向于尖锐的似然，优化 $L(\\theta)$ 找到既能解释数据又具有适当曲率的模型。\n📖方法论 # 所以总结一下，最后其实很简单——甚至损失函数只和 $p_m$ 有关。假设了 $p_m$ 的分布之后：\n先求其关于训练数据的一阶导数的平方（Norm 项） 再在 Norm 项算式的基础上求其二阶导（Hessian 项） 相加，求 $argmin_{\\theta}$，就完事儿了。Score matching by Andy Jones 的最后举了一个高斯的例子，很不错。 📚 延伸阅读 # Hyvärinen (2005): 原始论文，提出得分匹配。 Song \u0026amp; Ermon (2019): 使用得分匹配进行生成建模。 Sliced Score Matching (2020): 可扩展版本，适用于高维数据。 ","date":"21 July 2025","externalUrl":null,"permalink":"/posts/ai/aigc/awesome-blogs/score-matching-%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%8E%A8%E5%AF%BC/","section":"Posts","summary":"","title":"Score matching 的基本推导","type":"posts"},{"content":" About Me # Hi, this is Niyuta ✨\n🎓 Education # M.S. in Deep Learning @ Beihang University (BUAA) B.S. in Computer Science @ Beihang University (BUAA) 💻 Technical Stack # Languages \u0026amp; Tools:\nProficient: Python, Rust (perhaps) Familiar: Java, C#, JavaScript/TypeScript, C/C++, SQL Misc: Git, Docker, Linux, Qemu, Cloud, React, Vue, etc. 🔭 Current Interests # Machine Learning \u0026amp; Deep Learning Systems (Python 🐍) High-performance Computing (Rust 🦀) Operating Systems (Rust 🦀) Web Development (React \u0026amp; Vue \u0026amp; Go 🎉) 🎸 Beyond Code # Guitarist crafting riffs when not debugging Anime cosplayer bringing 2D to 3D Eternal student of math \u0026amp; physics ","date":"28 February 2019","externalUrl":null,"permalink":"/page/about/","section":"Pages","summary":"","title":"About","type":"page"},{"content":"","date":"28 February 2019","externalUrl":null,"permalink":"/page/","section":"Pages","summary":"","title":"Pages","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]